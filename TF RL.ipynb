{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nervanasys.com/demystifying-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time as timelib\n",
    "from scipy import signal\n",
    "from IPython import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startPath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "minibatch_size = 32 # As suggested in DeepMind Nature paper\n",
    "discount_rate = 0.99\n",
    "episode_max = 5000\n",
    "tepisode_max = 20\n",
    "random_action_probability= 0.9 # initial value of e in e-greedy exploration - 0.9\n",
    "exploration_period=100000\n",
    "target_reset = 10000 # Deepmind used 10000\n",
    "max_experience = 1000000\n",
    "controlbatch = 4\n",
    "learnrate = 0.00025 # Deepmind used 0.00025\n",
    "randomTC = 10000 #1000000 for nice and slow\n",
    "gradientmomentum = 0.95\n",
    "\n",
    "randommax = 20/controlbatch # multiplied by controlbatch\n",
    "emin = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "plot_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "L1 = 1.0\n",
    "L2 = 1.0\n",
    "M1 = 1.0\n",
    "M2 = 1.0\n",
    "model_K = 1.0\n",
    "model_C = 1.0\n",
    "model_f = 0.2\n",
    "model_Dem = 3.0\n",
    "G = 9.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "dt = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network. The outputs correspond to the predicted Q-values of the individual actions for the input state. The main advantage of this type of architecture is the ability to compute Q-values for all possible actions in a given state with only a single forward pass through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "# TensorFlow configuration\n",
    "session = tf.InteractiveSession()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= learnrate, decay=gradientmomentum)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learnrate)\n",
    "\n",
    "# Network\n",
    "observation_ind = 3\n",
    "observation_size = observation_ind*controlbatch\n",
    "output_size = 21\n",
    "hidden1_size = 100\n",
    "hidden2_size = 100\n",
    "observation = tf.placeholder(tf.float32, (None, observation_size), name=\"observation\")\n",
    "next_observation = tf.placeholder(tf.float32, (None, observation_size), name=\"next_observation\")\n",
    "\n",
    "with tf.name_scope(\"Live\") as scope:\n",
    "    \n",
    "    Layer1 = tf.Variable(tf.random_normal([observation_size, hidden1_size], stddev=0.35),name=\"Layer1\")\n",
    "    Layer2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size], stddev=0.35),name=\"Layer2\")\n",
    "    Layer3 = tf.Variable(tf.random_normal([hidden2_size, output_size], stddev=0.35),name=\"Layer3\")\n",
    "\n",
    "    y1 = tf.nn.relu(tf.matmul(observation, Layer1))\n",
    "    y2 = tf.nn.relu(tf.matmul(y1, Layer2))\n",
    "    y3 = tf.matmul(y2, Layer3)\n",
    "    \n",
    "    action_scores = tf.identity(y3, name=\"action_scores\") # Takes current value\n",
    "    predicted_actions = tf.argmax(action_scores, dimension=1, name=\"predicted_actions\")\n",
    "\n",
    "with tf.name_scope(\"Frozen\") as scope:\n",
    "    \n",
    "    TLayer1 = tf.Variable(Layer1.initialized_value(),name=\"TLayer1\")\n",
    "    TLayer2 = tf.Variable(Layer2.initialized_value(),name=\"TLayer2\")\n",
    "    TLayer3 = tf.Variable(Layer3.initialized_value(),name=\"TLayer3\")\n",
    "\n",
    "    Ty1 = tf.nn.relu(tf.matmul(next_observation, TLayer1))\n",
    "    Ty2 = tf.nn.relu(tf.matmul(Ty1, TLayer2))\n",
    "    Ty3 = tf.matmul(Ty2, TLayer3)\n",
    "\n",
    "    Taction_scores = tf.identity(Ty3, name=\"Taction_scores\") # Takes current value\n",
    "    Tpredicted_actions = tf.argmax(Taction_scores, dimension=1, name=\"predicted_actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Control parameters\n",
    "Cmax = 10\n",
    "C = np.linspace(-Cmax,Cmax,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training:  Placeholders\n",
    "action_mask = tf.placeholder(tf.float32, (None, output_size), name=\"action_mask\")\n",
    "rewards = tf.placeholder(tf.float32, (minibatch_size,), name=\"rewards\")\n",
    "next_observation_mask = tf.placeholder(tf.float32, (None,), name=\"next_observation_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training: Current\n",
    "masked_action_scores = tf.reduce_sum(action_scores * action_mask, reduction_indices=[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training: Future\n",
    "next_action_scores = tf.to_float(tf.stop_gradient(Taction_scores)) # Break tensor chain\n",
    "target_values = tf.reduce_max(next_action_scores, reduction_indices=[1,]) # Maximum predicted future reward\n",
    "future_rewards = tf.identity(rewards) + discount_rate * target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training: Optimisation\n",
    "with tf.name_scope(\"optimize\") as scope:\n",
    "    #temp_diff = tf.clip_by_value((masked_action_scores - future_rewards), -1, 1) # Clipped to -1 and 1\n",
    "    temp_diff = (masked_action_scores - future_rewards)\n",
    "    prediction_error = tf.square(temp_diff)\n",
    "    mean_prediction_error = tf.reduce_mean(prediction_error, reduction_indices=[0,]) # Mean across all samples per action\n",
    "    train_op = optimizer.minimize(temp_diff)\n",
    "    #mpe_summary = tf.scalar_summary(\"mean prediction error\", temp_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Writer\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter(\"Users/davidbrowne/tensorflowlogs/\", session.graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following previous approaches to playing Atari 2600 games, we also use a simple frame-skipping technique. More precisely, the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_ox = 0\n",
    "experience = deque()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the behaviour distribution is often selected by an e-greedy policy that follows the greedy policy with probability 1 - e and selects a random action with probability e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-de5913bb9530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEmSJEmSJID/B2LZI0VPkdxDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e6b4ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# More initialisation\n",
    "fix = 1\n",
    "control = 0\n",
    "time = 0\n",
    "episodes = 0\n",
    "state = np.array([model_Dem, 0.0, model_Dem])\n",
    "last_observation = np.zeros(controlbatch*observation_ind)\n",
    "rstate = 0\n",
    "experiencelog = deque()\n",
    "new_observation = state\n",
    "new_observationC = np.array([])\n",
    "new_action = np.floor(output_size/2)+1\n",
    "tv = []\n",
    "controls = []\n",
    "error = 0\n",
    "cheat = time_ox\n",
    "randomflag = 0\n",
    "randomcount = 0\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "for time_oxx in count():\n",
    "    time_ox = time_oxx + 1 + cheat\n",
    "    \n",
    "    if (time_ox % target_reset)==0:\n",
    "        TLayer1 = tf.identity(Layer1)\n",
    "        TLayer2 = tf.identity(Layer2)\n",
    "        TLayer3 = tf.identity(Layer3) # NB: Change this for 2 hidden layers\n",
    "        \n",
    "    time = time + dt\n",
    "    tv.append(time)\n",
    "    \n",
    "    # Saving previous step\n",
    "    last_action = new_action\n",
    "    \n",
    "    ## ----------------------------------------------------------------\n",
    "    ## INTERCHANGABLE MODEL\n",
    "    ## ----------------------------------------------------------------\n",
    "    # Model & Simulation\n",
    "    dydx = np.zeros_like(state)\n",
    "    dydx[0] = state[1]\n",
    "    dydx[1] = (-model_K*state[0] - model_C*state[1] + control) /M1\n",
    "    dydx[2] = 0\n",
    "    target = model_Dem #* signal.square(2 * np.pi * model_f * time)\n",
    "    \n",
    "    state = state + dydx * dt\n",
    "    state[2] = target\n",
    "    \n",
    "    # Collecting reward\n",
    "    preverror = error\n",
    "    error = (abs(state[0]-target))\n",
    "    \n",
    "    reward = -error +  np.sign(error-preverror)\n",
    "    \n",
    "    ## ----------------------------------------------------------------\n",
    "    \n",
    "    # Collecting together data for an observation\n",
    "    new_observation = state\n",
    "    \n",
    "    experiencelog.append((last_observation, last_action, reward, new_observation, target))\n",
    "    controls.append(control)\n",
    "    \n",
    "    # e-greedy control\n",
    "    n = time_ox\n",
    "    if n <= exploration_period:\n",
    "        exploration_p =  random_action_probability\n",
    "    else:\n",
    "        #exploration_p =  1 - (n * (1 - random_action_probability)) / (exploration_period)\n",
    "        exploration_p = np.max([random_action_probability-np.float((n-exploration_period))/1000000, emin])\n",
    "    \n",
    "    if randomcount > randommax:\n",
    "        randomflag = 0\n",
    "        randomcount = 0\n",
    "        \n",
    "    if (time_ox % (controlbatch))!=0:\n",
    "        new_observationC = np.append(new_observationC,new_observation)\n",
    "    else:\n",
    "        new_observationC = np.append(new_observationC,new_observation)\n",
    "        if randomflag == 0:\n",
    "            if (random.random() < exploration_p) | (time_ox<exploration_period):\n",
    "                new_action = random.randint(0, output_size - 1)\n",
    "                randomcount = 0\n",
    "                randomflag = 1\n",
    "            else:\n",
    "                new_actions = session.run( # Selecting control value\n",
    "                    predicted_actions,\n",
    "                    {observation: [new_observationC]}\n",
    "                 )\n",
    "                new_action = new_actions[0]\n",
    "        else:\n",
    "            randomcount += 1\n",
    "        \n",
    "        control = C[new_action]\n",
    "        #print control\n",
    "        # Collecting experience\n",
    "        #if time_ox > 1:\n",
    "        experience.append((last_observation, last_action, reward, new_observationC)) # Switched to last action for a logical set of data\n",
    "        \n",
    "        last_observation = new_observationC # Reset last observation now that it has been added to experience\n",
    "        new_observationC = np.array([])\n",
    "            \n",
    "        if len(experience) > max_experience:\n",
    "            experience.popleft()\n",
    "\n",
    "        # Training\n",
    "        if len(experience) > minibatch_size:\n",
    "            samples = random.sample(range(len(experience)), minibatch_size)\n",
    "            samples = [experience[i] for i in samples]\n",
    "\n",
    "            Xobservation = np.empty((len(samples), observation_size))\n",
    "            Xnewobservation = np.empty((len(samples), observation_size))\n",
    "            Xaction_mask = np.zeros((len(samples), output_size))\n",
    "            Xrewards = np.zeros((len(samples),))\n",
    "\n",
    "            for i, (x_observation, x_action, x_reward, x_newobservation) in enumerate(samples):\n",
    "                Xobservation[i] = x_observation\n",
    "                Xaction_mask[i][x_action] = 1\n",
    "                Xrewards[i] = x_reward\n",
    "                Xnewobservation[i] = x_newobservation\n",
    "            \n",
    "            #print Xaction_mask\n",
    "            #print Xrewards\n",
    "            \n",
    "            cost, _, = session.run([ # Fetches\n",
    "                    action_scores,\n",
    "                    train_op,\n",
    "                ], { \n",
    "                    observation: Xobservation,\n",
    "                    next_observation: Xnewobservation,\n",
    "                    action_mask: Xaction_mask,\n",
    "                    rewards: Xrewards,\n",
    "                })        \n",
    "\n",
    "            # Add to tensorboard\n",
    "            #writer.add_summary(summary, time_oxx)\n",
    "    \n",
    "    \n",
    "            # Hunting down NaNs\n",
    "            temp = Layer3.eval()\n",
    "\n",
    "            if np.isnan(temp).any():\n",
    "                print np.isnan(Xobservation)\n",
    "                print np.isnan(Xnewobservation)\n",
    "                print np.isnan(Xaction_mask)\n",
    "                print np.isnan(Xrewards)\n",
    "                break\n",
    "        \n",
    "    \n",
    "    # End of optimization\n",
    "    if episodes == episode_max:\n",
    "        break\n",
    "\n",
    "    if time > tepisode_max:\n",
    "        \n",
    "        # Plotting every X episodes\n",
    "        \n",
    "        states = [row[3] for row in experiencelog]\n",
    "        tgt = [row[4] for row in experiencelog]\n",
    "        x = [row[0] for row in states]\n",
    "        v = [row[1] for row in states]\n",
    "        rewardsTotal = sum([row[2] for row in experiencelog])\n",
    "        \n",
    "        if (episodes % plot_every) == 0:\n",
    "            print rewardsTotal, exploration_p, np.mean(x)\n",
    "            plt.cla()\n",
    "            axes.plot(tv,controls)\n",
    "            axes.plot(tv, x)\n",
    "            axes.plot(tv, v)\n",
    "            axes.plot(tv,tgt)\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            timelib.sleep(1.0)\n",
    "            #fix = fix + 1\n",
    "            \n",
    "        # Resetting for next episode\n",
    "        time = 0\n",
    "        tv = []\n",
    "        episodes = episodes + 1\n",
    "        state = np.array([model_Dem, 0.0, model_Dem])\n",
    "        new_action = np.floor(output_size/2)+1\n",
    "        new_observation = state\n",
    "        controls = []\n",
    "        time = 0\n",
    "        tv = []\n",
    "        episodes = episodes + 1\n",
    "        experiencelog = deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /Users/davidbrowne/TensorFlowReinforcementLearning/Models/ConstantDemandSpringMassDamper.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = os.path.join(startPath,'Models','ConstantDemandSpringMassDamper.ckpt')\n",
    "saver.save(session, save_path)\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
